{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/mlp.png\" alt=\"drawing\" width=\"500\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data. 2 example with 3 features.\n",
    "x = torch.tensor([[1, 0, 1],\n",
    "                  [2, 1, 0],\n",
    "                  [1, 0, 1],\n",
    "                  [2, 1, 0]], dtype=torch.float)\n",
    "y = torch.tensor([[1, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [0, 1]], dtype=torch.float)\n",
    "\n",
    "# weitght matrix of first layer. dim (3, 2)\n",
    "w1 = torch.tensor([[0.5, 0.2],\n",
    "                   [0.2, 0.3],\n",
    "                   [0.1, 0.1]])\n",
    "# First layer bias dim (2, )\n",
    "b1 = torch.tensor([[0.5, 0.7]])\n",
    "# Second layer weight matrix dim (2, 3)\n",
    "w2 = torch.tensor([[0.1, 0.2, 0.5],\n",
    "                   [0.1, 0.3, 0.1]])\n",
    "# Second layer bias dim (3, )\n",
    "b2 = torch.tensor([[0.3,0.6,0.7]])\n",
    "# weitght matrix of third layer. dim (3, 2)\n",
    "w3 = torch.tensor([[0.5, 0.2],\n",
    "                   [0.2, 0.3],\n",
    "                   [0.1, 0.1]])\n",
    "# Third layer bias dim (2, )\n",
    "b3 = torch.tensor([[0.5,0.7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass layer 1\n",
    "preact1_a1 = x @ w1 + b1\n",
    "postact1_h1 = torch.sigmoid(preact1_a1)\n",
    "# forward pass layer 2\n",
    "preact2_a2 = postact1_h1 @ w2 + b2\n",
    "postact2_h2 = torch.sigmoid(preact2_a2)\n",
    "# forward pass layer 3\n",
    "preact3_yhat = postact2_h2 @ w3  + b3\n",
    "yhat = torch.softmax(preact3_yhat, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**<br>\n",
    "Assume the problem is a k clas classifictaion problem. There are total N training sample, each sample has d features.<br>\n",
    "The complete calculation is there in the deep learning notevook<br>\n",
    "1. Loss Function: Cross Entropy loss ($L\\in R$)\n",
    "$$\n",
    "\\begin{align*}\n",
    "L&=-\\sum_{i=1}^N \\sum_{c=1}^k y_{i,c} log(\\hat{y}_{i,c})\\\\\\\n",
    "&=-\\sum_{i=1}^N log(\\hat{y}_{i,l}) \\text{where, l corrcsponds to the true class for the example i}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "2. Gradient corresponding to predicted value (output vector for $i^{th} example$) ($\\nabla_{\\hat{y_i}}L\\in R^k$)\n",
    "$$\n",
    "\\nabla_{\\hat{y_i}}L = -\\frac{1}{\\hat{y}_{i,l}}e_{i,l}   \\text{    where  }e_{i,l} \\in R^k=\\begin{cases}1 \\text{  if  } l=i\\\\0 \\text{ else } \\end{cases}\\\\\n",
    "$$\n",
    "Therefore, $\\nabla_{\\hat{y}}\\in R^{N\\times k}$, stack gradients corresponding to each example<br>\n",
    "3. Gradient corresponding to the preactivation part of final layer and $i^{th} example$($\\nabla_{a_i^L}\\in R^k$)\n",
    "$$\n",
    "\\nabla_{a_i^L} = \\hat{y}_i-e_{i, l}\n",
    "$$\n",
    "Therefore, $\\nabla_{a^L}\\in R^{N\\times k}$, stack gradients corresponding to each example<br>\n",
    "4. Gradient corresponding to activations of $i^{th}$ example $2^{nd}$ layer ($\\nabla_{h_i^2}\\in R^3$)\n",
    "In general $i^{th}$ example $j^{nd}$\n",
    "$$\n",
    "\\nabla_{h_i^j} = \\nabla_{a^{j+1}}LW^{j+1^{T}}\\\\\n",
    "\\nabla_{h_i^2} = \\nabla_{a^{3}}LW^{3^T}\n",
    "$$\n",
    "Therefore, $\\nabla_{h^2}\\in R^{N\\times 3}$, stack gradients corresponding to each example<br>\n",
    "5. Gradient corresponding to pre activation of $i^{th}$ example $2^{nd}$ layer ($\\nabla_{a_i^2}\\in R^3$)\n",
    "In general $i^{th}$ example $j^{nd}$\n",
    "$$\n",
    "\\nabla_{a_i^j} = \\nabla_{h^{i}}L\\odot g^{'}(a^i)\\\\\n",
    "\\nabla_{a^2_i} = \\nabla_{h^{2}_i}L\\odot g^{'}(a^2_i)\\\\\n",
    "$$\n",
    "Therefore, $\\nabla_{a^2}\\in R^{N\\times 3}$, stack gradients corresponding to each example<br>\n",
    "6. Gradient correspondint to weights ($\\nabla_{W^3}\\in R^{3\\times 2}$)\n",
    "$$\n",
    "\\nabla_{W^i}L=h^{(j-1)^T}\\nabla_{a^i}L\\\\\n",
    "\\nabla_{W^3}L=h^{(2)^T}\\nabla_{a^3}L\n",
    "$$\n",
    "7. Gradient correspondint to biases ($\\nabla_{b^3}\\in R^{2}$)\n",
    "$$\n",
    "\\nabla_{b^i}L=\\frac{1}{N}\\sum_{j=1}^{N}\\nabla_{a^i_j}L\\\\\n",
    "\\nabla_{b^3}L=\\frac{1}{N}\\sum_{j=1}^{N}\\nabla_{a^3_j}L\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1289)\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation\n",
    "loss = -torch.nn.functional.cross_entropy(y, torch.log(yhat))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient corresponding to the parameters of the final layer\n",
    "class_labels = torch.argmax(y, dim=1)   # Find the labels for each example.\n",
    "grad_yhat = torch.zeros_like(yhat)\n",
    "grad_yhat[torch.arange(len(class_labels)), class_labels] = -1/yhat[torch.arange(len(class_labels)), class_labels]\n",
    "# grad_yhat = torch.tensor([[0.48, 0],\n",
    "#                          [0, 0.52]])\n",
    "grad_a3 = yhat - y\n",
    "grad_w3 = postact2_h2.T @ grad_a3\n",
    "grad_b3 = grad_a3.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postact2_h2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient corresponding to the parameters of the final layer\n",
    "grad_h2 = grad_a3 @ w3.T\n",
    "grad_a2 = grad_h2 * preact2_a2\n",
    "grad_w2 = postact1_h1.T @ grad_a2\n",
    "grad_b2 = grad_a2.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8051e-03,  1.1965e-03, -6.4163e-09])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
