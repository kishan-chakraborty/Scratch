{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the first installment of transformers, with its basic building block called self attention mechanism.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Source] : 'https://www.youtube.com/watch?v=MVeOwsggkt4&list=PLZ2ps__7DhBZVxMrSkTIcG6zZBDKUXCnM&index=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages of the seq-seq model architecture like RNN and encoder-decoder in machine translation are.<br>\n",
    "<img src=\"Images/EncoderDecoder.png\" alt=\"drawing\" width=\"600\"/><br>\n",
    "1. Lack of context retention. Since, the encoder compress the input sentence into a single vector, complete essence of the input seq. may get lost.\n",
    "2. The RNN based architecture for encoding may suffer from lack of parallelization resulting in computational inefficiency.\n",
    "3. Lack of contextual representation. During encoding, no special attention given to dominant tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention mechanism and contextual learning.\n",
    "1. Once encoding completes, we can take the encoded internal state vectors, and feed them to the decoder for translation.<br>\n",
    "2. During translation, the weight corresponding to \"I\" and \"nan\" should be higher compared to other state vectors as shown in the alignment matrix.<br>\n",
    "3. The alignment score between state $s_t$ and $h_i$ can be given by\n",
    "$$\\alpha_{t,i}=align(s_t, h_i)=\\frac{exp(score(s_{t-1}, h_i))}{\\sum_{i'=1}exp(score(s_{t-1}, h_{i'}))}$$\n",
    "4. For a given t, all the $\\alpha_{t,i}$'s can be calculated in parallel but this cannot be done for all t's in parallel. This is because, the $alpha_{t,i}$ depends upond the value of $s_{t-1}$<br>\n",
    "5. The contextual learning paradigm of RNN also result in computational inefficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELF ATTENTION\n",
    "1. There is no recurrence relation in the self attention layers still it is aware of the contextual representation of the input sequence.\n",
    "2. The objective of the self attention layer is to take the current embeddings($h_i$'s) and find the contextual embeddings($s_i$'s).<br>\n",
    "The contextual embeddings of the word  \"movie\"($s_5$) is evaluated as attention weighted sum of the remaining words. Our objective is to evaluate these attention weights.\n",
    "$$\n",
    "    s_4 = \\sum_{j=1}^5 \\alpha_{4,j}h_j\n",
    "$$\n",
    "3. We can parallelize the calculation of $\\alpha_{i,j}$ because of no recurrence relation. We call it self attention because the attention depends only upon the input state vectors.\n",
    "4. Consider the sentence \"The animal didn't cross the street because it was too tired.\", here the contextual embedding of the word \"it\" <br>\n",
    "should have a higher weight corresponding to animal. If the last word is changed to congested, the contextual embedding should have higher weight<br>\n",
    "corresponding to road. Threfore, the alpha's should have higher values corresponding to these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is all you need\n",
    "<img src=\"Images/transformers.png\" alt=\"drawing\" width=\"300\"/><img src=\"Images/multihead_attention.png\" alt=\"drawing\" width=\"740\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT EMBEDDING\n",
    "1. vector representation of token in a sequence. In transformer, there is no concept of static embedding, it learns the tokenization while model training.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POSITIONAL ENCODING [the sinusoidal layer after the input embedding]<br>\n",
    "1. The purpose of the positional encoding layer is to make the model aware of the position of the input tokens in the sequence. <br>\n",
    "2. Since, the recurrence layer of the traditional seq-seq is removed, the contextual understanding is derived from the positon aware tokens<br>\n",
    "3. [TODO] Deep dive inside the positional encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELF ATTENTION\n",
    "1. Attention allows the model to focus on different part of the input token for model prediction. Multihead attention is concatenation of multiple self attention.<br>\n",
    "2. The 3 linear inputs shown in the above figure are three different representatio the same token. These are called, queries(Q), keys(K) and values(V).<br>\n",
    "3. The three representation of a single token is to learn different aspect of relationship between tokens in the same input sequence.<br>\n",
    "4. Query represents the token for which the attention weights are calculated. It assigns higher weights to the tokens which are more responsible for the prediction<br> corresponding to the current token.\n",
    "5. Key represents the other tokens in the sequence. It is responsible for calculating attention weights wrt to the query and determines the importance of other token in the seq.\n",
    "6. The value represents the content associated with the tokens in a sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for general attention mechanism in transformer model\n",
    "# Step1: position aware encoder representations of four different words. [deeper analysing required.]\n",
    "word_1 = np.array([1, 0, 0])\n",
    "word_2 = np.array([0, 1, 0])\n",
    "word_3 = np.array([1, 1, 0])\n",
    "word_4 = np.array([0, 0, 1])\n",
    "\n",
    "words = np.stack([word_1, word_2, word_3, word_4])      # matrix representation.\n",
    "words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tenosr word represents one sequence (batch dim.) of 4 tokens (time time dim.), each token is of dimension 3 (embedding dim.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the weight matrices\n",
    "np.random.seed(42)\n",
    "# The transformation: R3 -> R2\n",
    "W_Q = np.random.randn(3, 2)     # Query matrix\n",
    "W_K = np.random.randn(3, 2)     # Key matrix\n",
    "W_V = np.random.randn(3, 3)     # Value matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1.1: Generating the queries, keys and values\n",
    "query_1 = word_1 @ W_Q\n",
    "key_1 = word_1 @ W_K\n",
    "value_1 = word_1 @ W_V\n",
    " \n",
    "# Parallelizing the process\n",
    "Q = words @ W_Q     # Querries\n",
    "K = words @ W_K     # Keys\n",
    "V = words @ W_V     # Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step in the multi head self attention paradigm is to calculate the similarity score between querries and keys.\n",
    "score1 = np.array([Q[0].dot(K[0]), Q[0].dot(K[1]), Q[0].dot(K[2]), Q[0].dot(K[3])])     # The similarity score between query 1 and all the keys.\n",
    "\n",
    "# Step2 [MATMUL]: Parallelizing the process, we have the score matrix called the attention filter.\n",
    "# Assuming a MT task, for time step t, we want to calculate the influence of all the tokens. The influence is calculated as similarity score.\n",
    "score = Q@K.T       # score[i, j]: the similarity score (influence) of j'th key over the i'th time token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of Q 0.4635347453166358\n",
      "Variance of K 0.6331985417208494\n",
      "Variance of score 1.8052575483422657\n",
      "Variance of scor after scaling 0.9026287741711327\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Scaling the resultant attention filter. This is a design choice to manage the magnitude of the gradient during the training process.\n",
    "# This is to normalize the variance resltant after the matrix multiplication.\n",
    "# The scaling factor is square root of the dimension of key vector.\n",
    "print('Variance of Q', Q.var())\n",
    "print('Variance of K', K.var())\n",
    "print('Variance of score', score.var())\n",
    "score_scaled = score / np.sqrt(len(K[0]))\n",
    "print('Variance of scor after scaling', score_scaled.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The whole process of self attention mechanism can be done in a single line.\n",
    "def softmax(arr: np.array, axis: int) -> float:\n",
    "    weights = np.exp(arr) / np.exp(arr).sum()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Apply softmax function along the rows. Each row of the score matrix represents the similarity of a query to all the keys.\n",
    "# We want the weight matrix to normalize the similarity scores, hence apply softmax function along the rows to generate the weight matrix.\n",
    "weights = softmax(score_scaled, -1)      # why axis -1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: The attentions are calculated as an weighted sum of the value vectors.\n",
    "attention = weights@V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete process can be done in a single line<br>\n",
    "$$softmax(\\frac{QK^T}{\\sqrt{dim_k}})\\times V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing the above steps in a single line of code.\n",
    "attention = softmax(Q@K.T/np.sqrt(len(K[0])), -1)@V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Musking**<br>\n",
    "1. For encoder there is no requirement of masking, as all tokens in a seq influence each other tokens.\n",
    "1. During decoding we cannot use the future tokens to calculate the score matrix, therefore we need masking<br>\n",
    "    which can restrict the use of future tokens from influencing the current token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiheaded Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiheaded self attention is a process of concatenation of multiple self attention heads to form a long vector, which is the<br>\n",
    "input of the next step in the transformer architecture. The purpose of the multiple single head attention is that an individual <br>\n",
    "head focus on unique aspects of the semantics of the input sequence. After the concatenation the concatenaaetd vector is again <br>\n",
    "passed through another linear layer th shrink the size of the input vector.<br>\n",
    "<img src=\"Images/multihead_attention_eqs.png\" alt=\"drawing\" width=\"590\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 4 # No. of maximum words in a sequence. For seq with length less than max_seq_len, padding is required.\n",
    "batch_size = 1  # Batch size (no. of sequences) for parallel processing.\n",
    "input_dim = 3   # Dimension of the embedding vectors.\n",
    "d_k = 64        # Dimension of the linearly projected queries and keys.\n",
    "d_v = 64        # Dimension of the linearly projected valuess.\n",
    "d_model = 5     # Dimension of the output of the multihead attention layer.\n",
    "x = torch.randn((batch_size, max_seq_len, input_dim))   # Random input tensor\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 8\n",
    "head_dims = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/multihead_attention_unrolled1.png\" alt=\"drawing\" width=\"590\"/> <img src=\"Images/multihead_attention_unrolled2.png\" alt=\"drawing\" width=\"700\"/><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Implementing scaled dot procduct attention.\n",
    "        \"\"\"\n",
    "\n",
    "    def __call__(self, Qs: torch.tensor, Ks: torch.tensor, Vs: torch.tensor, mask: bool) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Calling scaled dot procduct attention.\n",
    "\n",
    "        Args:\n",
    "            Qs: query matrix    [batch_size, n_heads, max_seq_len, d_k]\n",
    "            Ks: key matrix      [batch_size, n_heads, max_seq_len, d_k]\n",
    "            Vs: values  matrix  [batch_size, n_heads, max_seq_len, d_v]\n",
    "            mask: whether to apply mask based on whether called during encoding/decoding.\n",
    "\n",
    "        Returns:\n",
    "            Calculated attention weights.\n",
    "        \"\"\"\n",
    "        score_mat = Qs @ Ks.permute(0, 1, 3, 2)\n",
    "        score_mat_scaled = score_mat / np.sqrt(len(Ks[0]))\n",
    "\n",
    "        if mask:\n",
    "            score_mat_scaled = score_mat_scaled.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention = F.softmax(score_mat_scaled, -1)\n",
    "        out =  attention @ Vs\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all the concept to build a multihead attention class using numpy\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, h: int, d_k: int, d_v: int, d_emb: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize multihead attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            h: number of heads\n",
    "            d_k: dimension of key and query vectors\n",
    "            d_v: dimension of value vector\n",
    "            d_emb: embedding dimension of each token.\n",
    "            max_seq_len: maximum sequence length allowed.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_heads = h                    # No. of heads.\n",
    "        self.d_k = d_k                      # key dimension.\n",
    "        self.d_v = d_v                      # value dimension.\n",
    "        self.d_model = h * d_v              # Model dimension.\n",
    "        self.W_Q = nn.Linear(d_emb, d_k*h)  # Query matrix.\n",
    "        self.W_K = nn.Linear(d_emb, d_k*h)  # Key matrix.\n",
    "        self.W_V = nn.Linear(d_emb, d_v*h)  # Value matrix.\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # Output matrix.\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention()\n",
    "\n",
    "    def split_head(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Split the input tensor into multiple heads.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor of shape (batch_size, max_seq_len, d_k*n_heads)\n",
    "\n",
    "        Returns:\n",
    "            Reshaped tensor of dimension (batch_size, n_heads, max_seq_len, d_k/d_v)\n",
    "        \"\"\"\n",
    "        x = x.view(x.shape[0], x.shape[1], self.n_heads, -1)    # (batch_size, max_seq_len, n_heads, d_k/d_v)\n",
    "        out = x.permute(0, 2, 1, 3)                             # (batch_size, n_heads, max_seq_len, d_k/d_v)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x, mask: bool) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of multihead attention.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor of shape (batch_size, max_seq_len, d_k*n_heads)\n",
    "            mask: whether to apply mask based on whether called during encoding/decoding.\n",
    "\n",
    "        Returns:\n",
    "            Reshaped tensor of dimension (batch_size, max_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        Qs = self.W_Q(x)       # Queries, shape: (batch_size, max_seq_len, d_k*n_heads)\n",
    "        Ks = self.W_K(x)       # Keys, shape: (batch_size, max_seq_len, d_k*n_heads)\n",
    "        Vs = self.W_V(x)       # Values, shape: (batch_size, max_seq_len, d_v*n_heads)\n",
    "\n",
    "        Qs = self.split_head(Qs)   # (batch_size, n_heads, max_seq_len, d_k)\n",
    "        Ks = self.split_head(Ks)   # (batch_size, n_heads, max_seq_len, d_k)\n",
    "        Vs = self.split_head(Vs)   # (batch_size, n_heads, max_seq_len, d_v)\n",
    "\n",
    "        multihead_vals = self.scaled_dot_product_attention(Qs, Ks, Vs, mask)   # (batch_size, n_heads, max_seq_len, d_v)\n",
    "        multihead_vals = multihead_vals.view(x.shape[0], x.shape[1], self.n_heads*self.d_v)   # (batch_size, max_seq_len, d_model)\n",
    "        out = self.W_o(multihead_vals)   # (batch_size, max_seq_len, d_model)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[NOTE]\n",
    "The proposed transformer architecture by Vaswani et al. has the same key and value dimension of 512.\n",
    "The number of heads is 8 hence the dimension of linearly projected queries, keys and values is going to be 64.\n",
    "The input embedding dimension is same as d_model = num_head*key_dim.\n",
    "\n",
    "To make a generalized architecture I have not hard coded the suggested values which makes it more generalizable.\n",
    "\"\"\"\n",
    "input_dim = 512\n",
    "d_k = 64\n",
    "d_v = 32\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
    "\n",
    "model = MultiheadAttention(num_heads, d_k, d_v, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (150x256 and 5x5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 60\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     58\u001b[0m multihead_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(Qs, Ks, Vs, mask)   \u001b[38;5;66;03m# (batch_size, n_heads, max_seq_len, d_v)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m multihead_vals \u001b[38;5;241m=\u001b[39m multihead_vals\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_v)   \u001b[38;5;66;03m# (batch_size, max_seq_len, d_model)\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_o\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmultihead_vals\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# (batch_size, max_seq_len, d_model)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\darsh\\Desktop\\Projects\\Scratch\\my_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\darsh\\Desktop\\Projects\\Scratch\\my_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\darsh\\Desktop\\Projects\\Scratch\\my_venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (150x256 and 5x5)"
     ]
    }
   ],
   "source": [
    "model.forward(x, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
