{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the first installment of transformers, with its basic building block called self attention mechanism.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Source] : 'https://www.youtube.com/watch?v=MVeOwsggkt4&list=PLZ2ps__7DhBZVxMrSkTIcG6zZBDKUXCnM&index=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages of the seq-seq model architecture like RNN and encoder-decoder in machine translation are.<br>\n",
    "<img src=\"Images/EncoderDecoder.png\" alt=\"drawing\" width=\"600\"/><br>\n",
    "1. Lack of context retention. Since, the encoder compress the input sentence into a single vector, complete essence of the input seq. may get lost.\n",
    "2. The RNN based architecture for encoding may suffer from lack of parallelization resulting in computational inefficiency.\n",
    "3. Lack of contextual representation. During encoding, no special attention given to dominant tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention mechanism and contextual learning.\n",
    "1. Once encoding completes, we can take the encoded internal state vectors, and feed them to the decoder for translation.<br>\n",
    "<img src=\"Images/attention1.png\" alt=\"drawing\" width=\"590\"/> <img src=\"Images/attention2.png\" alt=\"drawing\" width=\"600\"/><br>\n",
    "2. During translation, the weight corresponding to \"I\" and \"nan\" should be higher compared to other state vectors as shown in the alignment matrix.<br>\n",
    "3. The alignment score between state $s_t$ and $h_i$ can be given by\n",
    "$$\\alpha_{t,i}=align(s_t, h_i)=\\frac{exp(score(s_{t-1}, h_i))}{\\sum_{i'=1}exp(score(s_{t-1}, h_{i'}))}$$\n",
    "4. For a given t, all the $\\alpha_{t,i}$'s can be calculated in parallel but this cannot be done for all t's in parallel. This is because, the $alpha_{t,i}$ depends upond the value of $s_{t-1}$<br>\n",
    "5. The contextual learning paradigm of RNN also result in computational inefficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELF ATTENTION\n",
    "1. There is no recurrence relation in the self attention layers still it is aware of the contextual representation of the input sequence.\n",
    "2. The objective of the self attention layer is to take the current embeddings($h_i$'s) and find the contextual embeddings($s_i$'s).<br>\n",
    "The contextual embeddings of the word  \"movie\"($s_5$) is evaluated as attention weighted sum of the remaining words. Our objective is to evaluate these attention weights.\n",
    "$$\n",
    "    s_4 = \\sum_{j=1}^5 \\alpha_{4,j}h_j\n",
    "$$\n",
    "3. We can parallelize the calculation of $\\alpha_{i,j}$ because of no recurrence relation. We call it self attention because the attention depends only upon the input state vectors.\n",
    "4. Consider the sentence \"The animal didn't cross the street because it was too tired.\", here the contextual embedding of the word \"it\" <br>\n",
    "should have a higher weight corresponding to animal. If the last word is changed to congested, the contextual embedding should have higher weight<br>\n",
    "corresponding to road. Threfore, the alpha's should have higher values corresponding to these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is all you need\n",
    "<img src=\"Images/transformers.png\" alt=\"drawing\" width=\"300\"/><img src=\"Images/multihead_attention.png\" alt=\"drawing\" width=\"740\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT EMBEDDING\n",
    "1. vector representation of token in a sequence. In transformer, there is no concept of static embedding, it learns the tokenization while model training.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POSITIONAL ENCODING [the sinusoidal layer next to the input embedding]<br>\n",
    "1. The purpose of the positional encoding layer is to make the model aware of the position of the input tokens in the sequence. <br>\n",
    "2. Since, the recurrence layer of the traditional seq-seq is removed, the contextual understandin is derived from the positon aware tokens<br>\n",
    "3. [TODO] Deep dive inside the positional encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTIHEAD ATTENTION\n",
    "1. Attention allows the model to focus on different part of the input token for model prediction. Multihead attention allows multiple processes all at once.<br>\n",
    "2. The 3 linear inputs shown in the above figure are three self attention of the same token. These are called, queries(Q), keys(K) and values(V).<br>\n",
    "3. The three representation of a single token is learn different aspect of relationship between tokens in the same input sequence.<br>\n",
    "4. Query represents the token for which the attention weights are calculated. It assigns higher weights to the tokens which are more responsible for the prediction<br> corresponding to the current token.\n",
    "5. Key represents the other tokens in the sequence. It is responsible for calculating attention weights wrt to the query and determines the importance of other token in the seq.\n",
    "6. The value represents the content associated with the tokens in a sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for general attention mechanism in transformer model\n",
    "# Step1: position aware encoder representations of four different words. [deeper analysing required.]\n",
    "word_1 = np.array([1, 0, 0])\n",
    "word_2 = np.array([0, 1, 0])\n",
    "word_3 = np.array([1, 1, 0])\n",
    "word_4 = np.array([0, 0, 1])\n",
    "\n",
    "words = np.stack([word_1, word_2, word_3, word_4])      # matrix representation.\n",
    "words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the weight matrices\n",
    "np.random.seed(42)\n",
    "# The transformation: R3 -> R2\n",
    "W_Q = np.random.randint(3, size=(3, 2))     # Query matrix\n",
    "W_K = np.random.randint(3, size=(3, 2))     # Key matrix\n",
    "W_V = np.random.randint(3, size=(3, 3))     # Value matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1.1: Generating the queries, keys and values\n",
    "query_1 = word_1 @ W_Q\n",
    "key_1 = word_1 @ W_K\n",
    "value_1 = word_1 @ W_V\n",
    " \n",
    "# Parallelizing the process\n",
    "Q = words @ W_Q     # Querries\n",
    "K = words @ W_K     # Keys\n",
    "V = words @ W_V     # Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step in the multi head self attention paradigm is to calculate the similarity score between querries and keys.\n",
    "score1 = np.array([Q[0].dot(K[0]), Q[0].dot(K[1]), Q[0].dot(K[2]), Q[0].dot(K[3])])     # The similarity score between query 1 and all the keys.\n",
    "\n",
    "# Step2 [MATMUL]: Parallelizing the process, we have the score matrix called the attention filter.\n",
    "score = Q@K.T       # score[i, j]: the similarity score of i'th query with the j'th key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Scaling the resultant attention filter. This is a design choice to manage the magnitude of the gradient during the training process.\n",
    "# The scaling factor is square root of the dimension of key vector.\n",
    "score_scaled = score / np.sqrt(len(K[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Appky softmax function over the entire array not along a particular dimension. \n",
    "weights = np.exp(score_scaled) / np.exp(score_scaled).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: The attentions are calculated as an weighted sum of the value vectors.\n",
    "attention = weights@V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete process can be done in a single line<br>\n",
    "$$softmax(\\frac{QK^T}{\\sqrt{dim_k}})\\times V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The whole process can be done in a single line\n",
    "def softmax(arr: np.array) -> float:\n",
    "    weights = np.exp(arr) / np.exp(arr).sum()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(Q@K.T/np.sqrt(len(K[0])))@V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
